{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face detection using logistic regression\n",
    "\n",
    "Ok, but what does any of this have to do with computer vision? Well. We have already spoken about this idea of feature extraction and the manifold hypothesis, as well as introduced some data-driven matching approaches where we extract features from images and match to the manifolds these lie on. Lets use logistic regression to train a classify using some features.\n",
    "\n",
    "We'll explore this by modifying an early approach to face detection based on haar wavelets - the [Viola and Jones Algorithm](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf). This approach extracted a set of features from images using simple convolution filters (Haar wavelets) and then trained a set of decision trees to classify these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "import cv2\n",
    "\n",
    "from scipy.signal import convolve2d\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from jupyterquiz import display_quiz\n",
    "import json\n",
    "with open(\"./Questions_week_8.json\", \"r\") as file:\n",
    "    questions = json.load(file)\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function reads an image from a URL and displays it.\n",
    "def read_image_from_url(url):\n",
    "    img = Image.open(requests.get(url, stream=True).raw)\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get some training data\n",
    "im = read_image_from_url('https://raw.githubusercontent.com/riganxu/selfieBenchmark/master/SelfieImg/selfie.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting some training data\n",
    "\n",
    "I am going to use the world's largest selfie to scrape some training images. This is an interesting challenge for face detection: https://github.com/riganxu/selfieBenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll cheat by using the built in Viola and Jones face detector from opencv to find faces in this image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained classifier\n",
    "face_detector = cv2.CascadeClassifier(cv2.data.haarcascades +'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect faces\n",
    "gray_im = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "faces = face_detector.detectMultiScale(gray_im,1.1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw rectangle around the faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(im, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "# Display the output\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that was easy - got quite a few faces. Nowhere near as many as with more modern state of the art detectors though. Anyway, Let's try and reverse engineer the classifier by using these faces as training data to train our own classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop and resize all the detected faces in the image\n",
    "positive_training_examples = []\n",
    "pos_labels = []\n",
    "for (x, y, w, h) in faces:\n",
    "    positive_training_examples.append(cv2.resize(gray_im[y:y+h,x:x+w],(20,20)))\n",
    "    pos_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a few faces - always look at your training data\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(positive_training_examples[i],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make some negative training examples by randomly cropping other parts of the image\n",
    "negative_training_examples = []\n",
    "neg_labels = []\n",
    "for k in range(len(positive_training_examples)):\n",
    "    i = np.random.randint(gray_im.shape[0]-20)\n",
    "    j = np.random.randint(gray_im.shape[1]-20)\n",
    "    negative_training_examples.append(gray_im[i:i+20,j:j+20])\n",
    "    neg_labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a few non-faces\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(negative_training_examples[i],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, these images have a bit too much variety in them, so lets extract some features using some special image filters called haar wavelets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_x = np.repeat(np.hstack((np.ones((1,9)),np.zeros((1,9)))),18,axis=0)/(18*18)\n",
    "kernel_y = np.repeat(np.vstack((np.ones((9,1)),np.zeros((9,1)))),18,axis=1)/(18*18)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(kernel_x,cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(kernel_y,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[5:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ims = negative_training_examples+positive_training_examples\n",
    "training_labels = neg_labels+pos_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "training_features = []\n",
    "for im in training_ims:\n",
    "    # Our features will be the filter responses to each of these operations, reshaped into a long vector and concatenated\n",
    "    response_x = convolve2d(im.astype(np.float32),kernel_x,mode='valid').reshape(1,-1)\n",
    "    response_y = convolve2d(im.astype(np.float32),kernel_y,mode='valid').reshape(1,-1)\n",
    "    training_features.append(np.hstack((response_x,response_y)))\n",
    "    \n",
    "# Normalise features [0-1]\n",
    "training_features = np.vstack(training_features)\n",
    "norm_constant = np.max(training_features) # save this for later\n",
    "training_features = training_features/norm_constant\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our dataset into a training portion and a validation portion\n",
    "x_train,x_test,y_train,y_test = train_test_split(training_features,training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have split our training data into two portions. Let's visually look at these features we've extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.vstack(x_train)[np.array(y_train)==1,:],vmin=0,vmax=1,aspect=0.5)\n",
    "plt.ylabel('Training sample')\n",
    "plt.xlabel('Feature dimension')\n",
    "plt.title('Face features')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.vstack(x_train)[np.array(y_train)==0,:],vmin=0,vmax=1,aspect=0.5)\n",
    "plt.title('Non-face features')\n",
    "plt.ylabel('Training sample')\n",
    "plt.xlabel('Feature dimension')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. I can visualy see differences between the filter response to faces and non-faces! Let's train a logistic regression classify using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = LogisticRegression()\n",
    "cls.fit(np.vstack(x_train),np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = cls.predict(np.vstack(x_train))\n",
    "Accuracy = np.sum(y_pred_train==y_train)/len(y_train)\n",
    "print(\"Training Accuracy: \", Accuracy)\n",
    "\n",
    "y_pred_test = cls.predict(np.vstack(x_test))\n",
    "Accuracy = np.sum(y_pred_test==y_test)/len(y_test)\n",
    "print(\"Test Accuracy: \", Accuracy)\n",
    "print(Accuracy*len(y_test),\"out of\",len(y_test),\"unseen faces correctly detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not incredible performance, but this is about the dumbest thing we can try to do, two giant hand crafted filters on some random faces pulled from a single selfie image and logistic regression recognises >70% of the faces we didn't show our classifier. Let's see if this works on my own face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_im = read_image_from_url('https://engit.monash.edu/profiles/wp-content/uploads/2021/02/mb-1.jpg')[0:450,160:450,:]\n",
    "test_im = read_image_from_url('https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Anthony_Albanese_portrait_%28re-crop%29.jpg/500px-Anthony_Albanese_portrait_%28re-crop%29.jpg')[80:280,150:350,:]\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_im)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "gray_test_im = cv2.resize(cv2.cvtColor(test_im,cv2.COLOR_BGR2GRAY),(20,20))\n",
    "plt.imshow(gray_test_im,cmap='gray')\n",
    "plt.title('Gray 20x20 Michael Burke')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "response_x = convolve2d(gray_test_im.astype(np.float32),kernel_x,mode='valid').reshape(1,-1)\n",
    "response_y = convolve2d(gray_test_im.astype(np.float32),kernel_y,mode='valid').reshape(1,-1)\n",
    "test_feature = (np.hstack((response_x,response_y)))\n",
    "\n",
    "# Normalise by training set norm factor - our features must be processed exactly the same way for the classifier!\n",
    "test_feature = test_feature/norm_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_feature,vmin=0,vmax=1)\n",
    "plt.title('My face feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = cls.predict(test_feature)\n",
    "if test_label:\n",
    "    print('This is a face.')\n",
    "else:\n",
    "    print('This is not a face.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity\n",
    "\n",
    "* Try your own image and see if this dodgy face detector is really any good.\n",
    "* This detect assumes we had cropped boxes around the faces already. What strategy would you use if you had an image without the face already cropped and centred?\n",
    "* What kind of issues do you think our choice of training data would introduce if we tried to deploy this model?\n",
    "* How would you improve this classifier?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[7:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
