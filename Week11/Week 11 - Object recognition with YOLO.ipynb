{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_from_url(url):\n",
    "    img = Image.open(requests.get(url, stream=True).raw)\n",
    "    img.thumbnail((256,256), Image.LANCZOS)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\")  # auto-downloads weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at what is inside a YOLO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=model.model, \n",
    "        input_size=(16, 3, 640, 640),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to notice in this YOLO model summary\n",
    "\n",
    "### 1) End-to-end I/O: from pixels → dense predictions\n",
    "- **Input:** `[B, 3, 640, 640]` (here `B=16`)\n",
    "- **Output:** `[B, 84, 8400]`\n",
    "  - `8400` is the number of prediction locations (anchors / grid cells across feature maps).\n",
    "  - `84 = 4 + 80` (typical COCO setup): **4 box values** + **80 class scores** per location.\n",
    "  - The model is **fully convolutional**, so input resolution can change (with matching changes to the `8400` term).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2) The \"backbone → neck → head\" story is visible in the shapes\n",
    "You can point out the classic resolution pyramid:\n",
    "\n",
    "- Early layers downsample quickly:\n",
    "  - `640×640 → 320×320 → 160×160 → 80×80 → 40×40 → 20×20`\n",
    "- Increasing channels as spatial size shrinks:\n",
    "  - `3 → 16 → 32 → 64 → 128 → 256`\n",
    "\n",
    "This is the tradeoff: **lower resolution, richer features**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) C2f blocks = the workhorse (efficient feature reuse)\n",
    "You’ll see many `C2f` modules repeated.\n",
    "- These are **lightweight residual/partial-connections style blocks** designed to keep accuracy while reducing compute.\n",
    "- Repetition of `C2f` at multiple scales is what builds **feature richness without exploding parameters**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) SPPF = “cheap global context”\n",
    "`SPPF` (Spatial Pyramid Pooling - Fast) shows up near the deepest layer (`20×20`).\n",
    "- It uses multiple pooling operations to inject **multi-scale context**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Upsample + Concat = the neck (feature fusion)\n",
    "Look for this pattern:\n",
    "- `Upsample` (e.g. `20×20 → 40×40`, then `40×40 → 80×80`)\n",
    "- `Concat` (channel dimension grows, e.g. `256 + 128 → 384`)\n",
    "\n",
    "This is the **FPN/PAN-style fusion idea**:\n",
    "- **Deep features** (semantic) are merged with **shallow features** (detail)\n",
    "- Helps detect **both small and large objects**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6) Detect head is multi-scale (and appears “recursive” in the summary)\n",
    "`Detect (22)` appears many times because it consumes multiple feature maps internally.\n",
    "- YOLO heads typically predict at **multiple scales** (commonly around `80×80`, `40×40`, `20×20` for 640 input).\n",
    "- That’s why the model can detect **small objects** (high-res head) and **big objects** (low-res head).\n",
    "\n",
    "---\n",
    "\n",
    "### 7) DFL at the end = Distribution Focal Loss (a modern box trick)\n",
    "At the bottom you see:\n",
    "- `DFL (dfl) ... Trainable: False`\n",
    "- Some YOLO variants predict a **distribution over distances** instead of direct box coordinates.\n",
    "- This often improves localization quality.\n",
    "- It’s listed as non-trainable because it’s effectively a **fixed transformation layer** used during decoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url = \"http://cdn.cnn.com/cnnnext/dam/assets/200130092551-02-market-street-san-francisco-car-free-now.jpg\"\n",
    "# url = \"https://m.media-amazon.com/images/I/71xybHPToQL._AC_SL1500_.jpg\"\n",
    "\n",
    "url = \"https://www.monash.edu/__data/assets/image/0006/959496/clayton-campus-green-chemical-futures-building-exterior2017.jpg\"\n",
    "\n",
    "results = model(read_image_from_url(url))\n",
    "\n",
    "plt.imshow(results[0].plot()[:,:,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires a webcam to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame, stream=True)\n",
    "\n",
    "    for r in results:\n",
    "        frame = r.plot()\n",
    "\n",
    "    cv2.imshow(\"YOLO Live Demo\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece4076_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
