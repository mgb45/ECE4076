{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1ba61d5",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model (GMM) ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6424c-e04f-4250-a6ca-257f52cdf736",
   "metadata": {},
   "source": [
    "You need to have the following packages to work with this notebook\n",
    "- [numpy](https://anaconda.org/anaconda/numpy)\n",
    "- [matplotlib](https://anaconda.org/conda-forge/matplotlib)\n",
    "- [sklearn](https://scikit-learn.org/stable/install.html)\n",
    "- [torch](https://pytorch.org/get-started/locally/)\n",
    "- [torchvision](https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddae43-eb11-479a-b058-7d25dd522853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_moons\n",
    "from utils_GMM import plot_gmm, plot_kmeans\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from jupyterquiz import display_quiz\n",
    "import json\n",
    "with open(\"./Questions_week_7.json\", \"r\") as file:\n",
    "    questions = json.load(file)\n",
    "\n",
    "np.random.seed(seed=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e7c12",
   "metadata": {},
   "source": [
    "## Generating some synthetic data.\n",
    "\n",
    "We use [make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) from [scikit-learn](https://scikit-learn.org/stable/index.html) to generate our data. The method <font color='LimeGreen'>make_blobs</font> creates isotropic Gaussian blobs for clustering. We start with a simple case with two blobs, located at $\\boldsymbol{\\mu}_1 = \\begin{pmatrix} -2\\\\0 \\end{pmatrix}$   and $\\boldsymbol{\\mu}_2 = \\begin{pmatrix} 2\\\\0 \\end{pmatrix}$. We consider each blob to have a small variance (here $\\mathrm{std}=0.25$), hence clusters are compact to a great degree. We sample 250 data points from each blob. Study the snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc410776-517a-4cf0-96d7-2c694b8bc9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, _ = make_blobs(n_samples=[250,250], centers=np.array([[-2,0],[2,0]]), cluster_std=[0.25,0.25])\n",
    "plt.scatter(X1[:,0],X1[:,1],edgecolor=\"black\", s=100, color=\"cyan\")\n",
    "# set axes range\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65296f7e",
   "metadata": {},
   "source": [
    "Now we are going to use GMM from the [scikit-learn](https://scikit-learn.org/stable/index.html) to cluster our synthetic data. We will use the [fit](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit) method to fit a GMM to our data. Study the snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm1 = GaussianMixture(n_components=2).fit(X1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d93c75b8",
   "metadata": {},
   "source": [
    "## Activity 1\n",
    "\n",
    "All information about the fitted GMM can be accessed via its <font color='LimeGreen'>weights_</font> , \n",
    "<font color='LimeGreen'>means_</font> , and  <font color='LimeGreen'>covariances_</font>\n",
    " attributes. Before checking these values, answer the following questions\n",
    "- what do you expect to see as the weights of the components in the mixture? Recall that for a GMM $\\sum_{i=1}^K w_i =1$.\n",
    "- what do you expect to see as the centers of the Gaussian components in the mixture?\n",
    "- what do you expect to see as the covariance matrix of the Gaussian components?\n",
    "\n",
    "\n",
    "<font color='orange'>  \n",
    "Now print out the above attributes. Does the result make sense\n",
    "</font>\n",
    "<font color='red', size=4> ? </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[0:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4bb4b23",
   "metadata": {},
   "source": [
    "## Activity 2\n",
    "\n",
    "hmm, this looks amazing. The GMM knew nothing about how we created our data, yet it recovered the underlying structure nicely. Let's make things a bit more interesting. How about creating a blob with 450 samples and higher dispersion (i.e., std), while having a small blob with only 50 samples and a low dispersion. \n",
    "\n",
    "\n",
    "<font color='orange'>  \n",
    "Use make_blobs to create such data, plot the data and then fit a GMM to it.\n",
    "</font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28b438a2",
   "metadata": {},
   "source": [
    "The accompanying <font color='LimeGreen'>utils_GMM.py</font> has a function called <font color='LimeGreen'>plot_gmm</font> to plot a learned GMM. We will use it below to plot the GMM learned for the second activity. Note that the GMM model in  [scikit-learn](https://scikit-learn.org/stable/index.html) has a method called [predict](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.predict) that you can use to identify the component with maximum likelihood given your data. In a sense, the predict method does a hard-clustering based on the weights and likelihood of Gaussian components. The <font color='LimeGreen'>plot_gmm</font> function use this method to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "plot_gmm(gmm2, X2, ax=ax1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4345f",
   "metadata": {},
   "source": [
    "## Activity 3\n",
    "\n",
    "In the $\\mathbb{R}^2$ plane, there are points that are more similar to the first component of our GMM and there are points that are more similar to teh second component. Maybe there are some points where the similarity to both components match and hence ambiguity arises. The method [predict_proba](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.predict_proba) can be used to obtain the likelihood of a sample with respect to each componenet as \n",
    "\\begin{align}\n",
    "    \\gamma_k(\\boldsymbol{x}) = \\frac{w_k \\mathcal{N}(\\boldsymbol{x}\\vert \\boldsymbol{\\mu}_k, \\Sigma_k)}\n",
    "    {\\sum_{j=1}^K\\limits w_j \\mathcal{N}(\\boldsymbol{x}\\vert \\boldsymbol{\\mu}_j, \\Sigma_j)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- Use meshgrid and traverse $[-6,6] \\times [-6,6]$ with an appropriate step and visualize the response of each component to the points in $[-6,6] \\times [-6,6]$  \n",
    "- Is the result expected? Can you explain what you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34cfd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_var = np.linspace(-6,6,500)\n",
    "var_x, var_y = np.meshgrid(region_var,region_var)\n",
    "plt_X = np.vstack([var_x.flatten(),var_y.flatten()]).T\n",
    "\n",
    "##### ADD YOUR CODE BELOW BY UNCOMMENTING THE LINE #####\n",
    "\n",
    "# resp_plt_X =\n",
    "\n",
    "########################################################\n",
    "\n",
    "# below we plot the probabilities wrt one of the components only (that serves our purpose)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "ax1.scatter(var_x, var_y, c=resp_plt_X[:,0].reshape(var_x.shape))\n",
    "ax1.scatter(X2[:,0], X2[:,1], edgecolor=\"black\", s=100, color=\"cyan\")\n",
    "# we can use the contour function to obtain the decision boundary as follows\n",
    "ax2.contour(var_x, var_y, resp_plt_X[:,0].reshape(var_x.shape)-resp_plt_X[:,1].reshape(var_x.shape), levels=[0])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df0b92",
   "metadata": {},
   "source": [
    "## Activity 4\n",
    "\n",
    "Let's make a more complicated case. We first generate 4 compact clusters and then multiply the sampled points with a random matrix $\\boldsymbol{A}$. You may ask why, let's see what will happen doing this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_0 = np.empty((0,2))\n",
    "X3 = np.empty((0,2))\n",
    "for tmp in range(3):\n",
    "    dummy_var, _ = make_blobs(n_samples=100, centers=1, cluster_std=0.75, center_box=(-5,5))\n",
    "\n",
    "    A = np.random.randn(2,2)\n",
    "\n",
    "    dummy_var2 = np.matmul(dummy_var,A)\n",
    "\n",
    "    X3_0 = np.append(X3_0,dummy_var,axis=0)\n",
    "    X3 = np.append(X3,dummy_var2,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# below we plot the our data before and after applying random linear projections\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "ax1.scatter(X3_0[:,0], X3_0[:,1], edgecolor=\"black\", s=100, color=\"cyan\")\n",
    "ax2.scatter(X3[:,0], X3[:,1], edgecolor=\"black\", s=100, color=\"cyan\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c0d68e",
   "metadata": {},
   "source": [
    "### - Now apply GMM to X3 and plot the result. Are you still happy with the result?\n",
    "- do you think K-Means algorithm can do a good job with this data? To test K-Means, use the function plot_kmeans from the <font color='LimeGreen'>utils_GMM.py</font>. You need to use this function as <font color='MediumSlateBlue'>plot_kmeans(X3, n_clusters=3)</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36187eaa",
   "metadata": {},
   "source": [
    "## Activity 5 \n",
    "\n",
    "Now we are going to push the limits. We will use the so-called two-moon dataset below. Use the knowledge you have gained and see if there is a way to cluster data where each arc can be grouped to a cluster or a set of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5790a-32db-4fed-8a5e-01164f522555",
   "metadata": {},
   "outputs": [],
   "source": [
    "X4, _ = make_moons(200, noise=.05, random_state=0)\n",
    "plt.scatter(X4[:, 0], X4[:, 1], edgecolor=\"black\", s=100, color=\"cyan\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda6a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[5:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf375c1",
   "metadata": {},
   "source": [
    "## So what - what does this mean for images?\n",
    "\n",
    "Let's build a generative model using the fashion mnist dataset. To do this we are going to introduce Principal Component Analysis, a nice application of singular value decomposition techniques we have seen earlier in the unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a685e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload some data and turn into numpy arrays\n",
    "data = torchvision.datasets.FashionMNIST('./data/',download=True)\n",
    "images = np.array(data.data)\n",
    "labels = np.array(data.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25981077",
   "metadata": {},
   "source": [
    "Fashion mnist comprises 60000 simple images of clothing, in 10 classes. It's a nice toy dataset for testing classifiers, but we will use it to build a simple generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a random subset of images - always look at your data\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(images[i,:,:],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60802b3",
   "metadata": {},
   "source": [
    "I don't have enough memory to work with the whole dataset at once, so will just use a subset of images from class 2. Principal component analysis finds a decomposition of images into a set of orthogonal vectors and coefficients, ordered in terms of their contribution to the original signal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't have enough memory, so I will just used a subset of all the images from class 2\n",
    "images = images[labels==2,:,:]\n",
    "images = images/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d935c80",
   "metadata": {},
   "source": [
    "The method is simple, subtract the mean of your data, then turn it into one long vector, and compute the SVD. This technique was originally proposed for face recognition - a method known as [Eigenfaces](https://sites.cs.ucsb.edu/~mturk/Papers/mturk-CVPR91.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centre all our our images by subtracting the mean, and reshape into 1 long vector\n",
    "Xm = np.mean(images,axis=0)\n",
    "\n",
    "X = (images - Xm).reshape(images.shape[0],-1)\n",
    "\n",
    "#Take the SVD of this matrix\n",
    "U,S,V = np.linalg.svd(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82afd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the shape of these matrices\n",
    "print(U.shape) #We have image number x image number - we'll call these basis functions\n",
    "print(V.shape) #We have pixel number x pixel number - we'll call these loadings or coefficients\n",
    "print(S.shape) #We have image number singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9909db7",
   "metadata": {},
   "source": [
    "Remember that the SVD decomposed a matrix into 2 orthogonal matrices, and a diagonal matrix of singular values S? The magnitude of these singular values tells us how much each of the vectors contributes to the original matrix.  We can exploit this to get a low dimensional approximation to the original images, by retaining only the 1st N vectors corresponding to the largest singular values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(S)\n",
    "plt.grid()\n",
    "plt.title('Singular values')\n",
    "plt.xlabel('Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc65365",
   "metadata": {},
   "source": [
    "To reconstruct our image using only a subset of components, we will use the following approach, taking only the first N basis functions, columns of U, and corresponding singular values and coefficients.\n",
    "\n",
    "`Xrecon = U[:,0:N]@S[:N]@V[:N,:]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,6,1)\n",
    "plt.imshow(images[0,:,:],cmap='gray')\n",
    "plt.title('Original image')\n",
    "\n",
    "# Reconstruct image using increasing numbers of basis functions\n",
    "for j,N in enumerate(np.arange(1,500,100)):\n",
    "    Xrecon = U[0,0:N]@np.diag(S[:N])@V[:N,:]\n",
    "    plt.subplot(1,6,j+2)\n",
    "    plt.imshow(Xrecon.reshape(28,28) + Xm,cmap='gray')\n",
    "    plt.title('N=%d'%N)\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ff86d",
   "metadata": {},
   "source": [
    "This is super cool. Singular value decomposition does compression (or feature extraction). Our original image comprised 784=28*28 pixels, but this shows we can get something looking a lot like our original image using about 100 components. This corresponds to storing a 100x784 matrix $\\hat{V}$, which is *image independent*, a 1x100 matrix of singular values, also *image independent*. The only image dependent information we kept was a 1x100 feature vector, the rows of U. So we have effectively compressed our image from 784 pixels to 100 d vector. We can also compress unseen image vectors $(I_v)$ by projecting an image into this space.\n",
    "\n",
    "$$U_{new} = I_v{(\\hat{S}\\hat{V})}^{-1}$$\n",
    "\n",
    "Intuitively, this makes sense, look at the curve above, 100 components is basically the *knee* of the curve, which like our method of choosing gaussian mixture components, seems a sensible trade-off between compression and reconstruction quality. \n",
    "\n",
    "Ok. Lets fit a Gaussian mixture model to these low dimensional vectors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32747daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "Ncomps = 25 #  I am going to arbitrarily choose 25 components - you may want to consider reducing this for memory.\n",
    "gmm_mnist = GaussianMixture(n_components=Ncomps).fit(U[:,:N].reshape(U.shape[0],-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8c413",
   "metadata": {},
   "source": [
    "Lets reconstruct the means of the GMM - cool, these look like different styles of shirt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b622699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "for j in range(Ncomps):\n",
    "    plt.subplot(10,5,j+1)\n",
    "    mu = gmm_mnist.means_[j]\n",
    "    # Notice that when we reconstruct, I clip the image to remain within the original bounds\n",
    "    im_recon = np.clip((mu@np.diag(S[:N])@V[:N,:]).reshape(28,28) + Xm,0,1)\n",
    "    plt.imshow(im_recon,cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace=None, hspace=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed0ab8",
   "metadata": {},
   "source": [
    "Lets draw some samples from these Gaussians. I will use a trick to sample from a multi dimensional Gaussian - cholesky decomposition. Recall that sampling from a one dimensional gaussian is\n",
    "\n",
    "$$ x_s = \\mu + \\sqrt(\\sigma^2) \\epsilon , \\epsilon \\sim{\\mathcal{N}(0,1)} $$\n",
    "\n",
    "Cholesky decomposition factorises a positive definite real matrix  $A = LL^{T}$ - like a matrix square root. We can use this to sample from a multi dimensional Gaussian with full covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(25):\n",
    "        idx = np.random.choice(Ncomps,1,p=gmm_mnist.weights_) #Sample a gaussian in proportion to the weights\n",
    "\n",
    "        sample = gmm_mnist.means_[idx] + np.random.randn(1,N)@np.linalg.cholesky(gmm_mnist.covariances_[idx]) #Sample from the this gaussian\n",
    "\n",
    "        plt.subplot(5,5,j+1)\n",
    "        # Show vector\n",
    "        im_recon = np.clip((sample@np.diag(S[:N])@V[:N,:]).reshape(28,28) + Xm,0,1)\n",
    "        plt.imshow(im_recon,cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3075794",
   "metadata": {},
   "source": [
    "Nice, we have built 'generative AI'. Modern techniques don't work this way, but do share some similarities. \n",
    "* They typically generate from a lower-dimensional representation or feature set (often learned using a neural network)\n",
    "* They often rely on gaussian mixture outputs [(mixture density networks)](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf) to provide a nice way of sampling (gpt models do something like this - also known as a mixture of expert model).  \n",
    "\n",
    "The ideas behind PCA are also very related to autoencoders. We will have a bonus notebook on these later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537b3c3",
   "metadata": {},
   "source": [
    "## Activity 6\n",
    "\n",
    "* Play around with the code above and observe the effects of reducing/ increasing the number of components used for dimension reduction and Gaussian mixture model fitting. What does this do to the quality of the images generated?\n",
    "* Try expand the model to use multiple image classes. How does this affect the quality of the generated images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74780498",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[6:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f022a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece4076_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
