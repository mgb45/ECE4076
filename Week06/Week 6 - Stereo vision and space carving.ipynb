{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The fundamental matrix and 3D reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1:\n",
    "\n",
    "Watch the fundamental matrix song for a brief recap of this weeks lecture videos. https://www.youtube.com/watch?v=DgGV3l82NTk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Let's try to understand this video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1 = cv2.imread('../test_images/lego_1.jpg')[:,:,[2,1,0]]\n",
    "im_2 = cv2.imread('../test_images/lego_2.jpg')[:,:,[2,1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im_1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate orb\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Find the keypoints and descriptors\n",
    "kp1, des1 = orb.detectAndCompute(im_1,None)\n",
    "kp2, des2 = orb.detectAndCompute(im_2,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match keypoints\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1,des2)\n",
    "matches = sorted(matches, key = lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw all matches\n",
    "plt.figure(figsize=(15,5))\n",
    "img3 = cv2.drawMatches(im_1,kp1,im_2,kp2,matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find keypoint correspondences\n",
    "X1 = np.vstack([kp1[match.queryIdx].pt for match in matches])\n",
    "X2 = np.vstack([kp2[match.trainIdx].pt for match in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find fundamental matrix using OpenCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-point algorithm approach\n",
    "# In general, it's harder to get a robust estimate of F, so good practices are, normalise image coordinates \n",
    "# Build matrix A from correspondences, extract F using SVD\n",
    "# Compute SVD of F\n",
    "# Enforce zero determinant by setting last singular value to zero\n",
    "# Recompute F using USV^T\n",
    "# De-normalise F\n",
    "\n",
    " # This is a fancier iterative method using Levenberg-Marquardt optimisation to minimise epipolar error\n",
    "F, mask = cv2.findFundamentalMat(X1,X2,cv2.FM_LMEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines_and_points(im_1,lines,X):\n",
    "    cols = ['C0','C1','C2','C3','C4','C5']\n",
    "    i = 0\n",
    "    for r,pt in zip(lines[:5],X[:5]):\n",
    "        c = cols[i]\n",
    "        x0,y0 = map(int, [0, -r[2]/r[1] ])\n",
    "        x1,y1 = map(int, [im_1.shape[1], -(r[2]+r[0]*im_1.shape[1])/r[1] ])\n",
    "        plt.plot([x0,x1],[y0,y1],'-o',color=c)\n",
    "        plt.plot([pt[0]],[pt[1]],'o',color=c)\n",
    "        i = i +1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paraphrasing the song: Post-multiplying x by F results in an epipolar lines). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines2 = F@np.hstack([X1,np.ones((X1.shape[0],1))]).T\n",
    "lines1 = F.T@np.hstack([X2,np.ones((X2.shape[0],1))]).T\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im_2)\n",
    "\n",
    "draw_lines_and_points(im_2,lines2.T,X2)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im_1)\n",
    "draw_lines_and_points(im_1,lines1.T,X1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epipoles are in the left and right null spaces of F. All epipolar lines pass through the epipoles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,Vt = np.linalg.svd(F)\n",
    "epipole_1 = Vt[-1,:]/Vt[-1,-1]\n",
    "\n",
    "U,S,Vt = np.linalg.svd(F.T)\n",
    "epipole_2 = Vt[-1,:]/Vt[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im_1)\n",
    "draw_lines_and_points(im_1,lines1.T,X1)\n",
    "\n",
    "plt.plot(epipole_1[0],epipole_1[1],'ko')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im_2)\n",
    "draw_lines_and_points(im_2,lines2.T,X2)\n",
    "\n",
    "plt.plot(epipole_2[0],epipole_2[1],'ko')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume I have already calibrated my camera, and have intrinsic matrix \n",
    "\n",
    "$K = \\begin{bmatrix}f & 0 & x_c\\\\0 & f & y_c\\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "We know that the essential matrix $\\mathbf{E}$ is related to the fundamental matrix as\n",
    "\n",
    "$\\mathbf{F} = \\mathbf{K_2}^{-\\text{T}} \\mathbf{E} \\mathbf{K_1}^{-1}$\n",
    "\n",
    "so could solve for $\\mathbf{E}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K =  np.array([[982.58476312, 0., 611.35192207],\n",
    "             [0., 985.12962747, 436.11543958],\n",
    "             [0., 0., 1.]])\n",
    "\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = K.T@F@K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's decompose the matrix E to try to solve for the rotation and translation of the second camera viewpoint, relative to the first. In our notes we saw that the essential matrix could be composed as\n",
    "\n",
    "$\\mathbf{E} = \\begin{bmatrix} t_1 \\\\ t_2 \\\\ t_3 \\end{bmatrix} \\times \\mathbf{R} $\n",
    "\n",
    "We want to reverse this, to solve for t and R. We also saw in the notes that we can write a cross product as matrix multiplication\n",
    "\n",
    "$\\mathbf{E} = \\begin{bmatrix} 0 & -t_3 & t_2 \\\\ t_3 & 0 & -t_1 \\\\ -t_2 & t_1 & 0 \\end{bmatrix} \\mathbf{R}$\n",
    "\n",
    "The SVD decomposes E into two orthogonal matrices and a diagonal matrix of singular values, and for essential matrices the last singular value is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1, R2, t = cv2.decomposeEssentialMat(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some non-examinable material (what happens inside this function)\n",
    "We can then use this and some properties of the SVD to get some possible decompositions into t and R. Unfortunately, there are multiple possible solutions to this decomposition. We have to check all of these, and choose the correct one (usually the one with the points in front of the camera - sometimes called chierality constraints).\n",
    "\n",
    "The pose recovery theorem says:\n",
    "\n",
    "$$ \\mathbf{E} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\text{T} $$\n",
    "\n",
    "$$ (\\mathbf{t}_1,\\mathbf{R}_1) = \\left(\\mathbf{U}\\mathbf{R}_z(+\\frac{\\pi}{2})\\mathbf{\\Sigma}\\mathbf{U}^\\text{T},\\mathbf{U}\\mathbf{R}_z(+\\frac{\\pi}{2})\\mathbf{V}^\\text{T}\\right)$$\n",
    "$$ (\\mathbf{t}_2,\\mathbf{R}_2) = \\left(\\mathbf{U}\\mathbf{R}_z(-\\frac{\\pi}{2})\\mathbf{\\Sigma}\\mathbf{U}^\\text{T},\\mathbf{U}\\mathbf{R}_z(-\\frac{\\pi}{2})\\mathbf{V}^\\text{T}\\right)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\mathbf{R}_z(+\\frac{\\pi}{2}) = \\begin{bmatrix}0 & -1 & 0\\\\ 1 & 0 & 0\\\\ 0 & 0 & 1\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2_1 = K@np.hstack([R1,t.reshape(-1,1)]) # Second camera view\n",
    "P2_2 = K@np.hstack([R2,t.reshape(-1,1)])\n",
    "\n",
    "P1 = K@np.hstack([np.eye(3), np.zeros((3,1))]) # First camera view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_3d = cv2.triangulatePoints(P1,P2_1,X1.T,X2.T)\n",
    "points_3d = points_3d/points_3d[-1,:]\n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "ax = plt.axes(projection='3d')\n",
    "plt.plot(points_3d[0,:],points_3d[1,:],points_3d[2,:],'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_3d = cv2.triangulatePoints(P1,P2_2,X1.T,X2.T)\n",
    "points_3d = points_3d/points_3d[-1,:]\n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot(points_3d[0,:],points_3d[1,:],points_3d[2,:],'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These points look vaguely sensible, but are quite sparse, so it's hard to tell. \n",
    "\n",
    "This is a problem with feature-based reconstruction. Active camera approaches like structured light approaches attempt to address this sparsity by projecting a pattern into a scene while imaging, to make it easier to find correspondences, particularly in textureless scenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2\n",
    "\n",
    "If we wanted to avoid this sparsity and perform dense stereo reconstruction, what constraints could we exploit? \n",
    "\n",
    "- Assume a parallel camera pair and list a few possible ways to address the correspondence problem.  \n",
    "\n",
    "The approach above gave us a sparse 3D reconstruction, but also the camera pose between two views.\n",
    "\n",
    "- Discuss how you could track how a camera has moved in space (visual odometry)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Carving\n",
    "\n",
    "As an alternative to triangulation, we could try space carving. Here, we'll start with a 3D voxel grid, project this into a scene, and keep only those pixels that agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_1 = im_1[:,:,0]<100\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(sil_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "xx,yy,zz = np.meshgrid(np.linspace(-1,1,N),np.linspace(-1,1,N),np.linspace(-0.1,-1.5,N))\n",
    "X = np.vstack((xx.ravel(),yy.ravel(),zz.ravel(),np.ones(N*N*N)))\n",
    "Occupancy = np.zeros([N,N,N],dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project voxels into first image\n",
    "xim = P1@X\n",
    "xim = xim/xim[2,:]\n",
    "\n",
    "# Get voxels in camera field of view\n",
    "bins_im = ((xim[0,:]>0)&(xim[0,:]<im_1.shape[0])&(xim[1,:]>0)&(xim[1,:]<im_1.shape[1]))\n",
    "coords = xim[0:2,bins_im].copy().astype(int)\n",
    "\n",
    "# Only voxels projecting onto silhoutte can be occupied \n",
    "Occupancy.ravel()[bins_im] = sil_1[coords[0,:],coords[1,:]]\n",
    "\n",
    "# Every 10th point projected onto image\n",
    "plt.plot(coords[1,::10],coords[0,::10],'o',alpha=0.1)\n",
    "plt.imshow(sil_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,9))\n",
    "ax = plt.axes(projection='3d')\n",
    "colours = np.empty(Occupancy.shape, dtype=object)\n",
    "colours[Occupancy] = 'blue'\n",
    "ax.voxels(Occupancy, facecolors=colours, edgecolor='k')\n",
    "ax.view_init(60,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture videos, by adding multiple views from different angles, we could continue to carve out the voxels, until only those attached to our object remain. Here's a nice computerphile [overview](https://www.youtube.com/watch?v=cGs90KF4oTc) and a nice [tutorial](https://blogs.mathworks.com/loren/2009/12/16/carving-a-dinosaur/?doing_wp_cron=1644893843.8964691162109375000000) describing this approach. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
