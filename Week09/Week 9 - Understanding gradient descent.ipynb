{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> ECE4076/5176 - Week 9 </h1>\n",
    "<h1 align=\"center\"> Understanding the Gradient Descent Algorithm </h1>\n",
    "\n",
    "\n",
    "\n",
    "You need to have the following packages to work with this notebook\n",
    "- [numpy](https://anaconda.org/anaconda/numpy)\n",
    "- [matplotlib](https://anaconda.org/conda-forge/matplotlib)\n",
    "- [sklearn](https://scikit-learn.org/stable/install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from jupyterquiz import display_quiz\n",
    "import json\n",
    "with open(\"./Questions_week_9.json\", \"r\") as file:\n",
    "    questions = json.load(file)\n",
    "\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Himmelblau function\n",
    "\n",
    "In this demo, we are interested in minimizing the Himmelblau function. The  [Himmelblau](https://en.wikipedia.org/wiki/Himmelblau%27s_function) function is\n",
    "used to test the performance of optimization algorithms and is named after David Himmelblau (1924â€“2011), who introduced it.\n",
    "\n",
    "\n",
    "The  function is defined as:\n",
    "\\begin{align} \n",
    "z = (x^2+y-11)^2 + (x+y^2-7)^2\\;. \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Below we define the function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Himmelblau_func(x,y):\n",
    "    z = (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by plotting the function so we can understand it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plt_array = np.linspace(-6,6,100)\n",
    "y_plt_array = np.linspace(-6,6,100)\n",
    "x_mesh, y_mesh = np.meshgrid(x_plt_array,y_plt_array)\n",
    "z_mesh = Himmelblau_func(x_mesh,y_mesh)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(x_mesh, y_mesh, z_mesh, cmap=\"jet\", rstride=1, cstride=1)\n",
    "plt.xlabel(r\"$x$\", fontsize=14)\n",
    "plt.ylabel(r\"$y$\", fontsize=14)\n",
    "plt.xticks([-6, -3, 0, 3, 6])\n",
    "plt.yticks([-6, -3, 0, 3, 6])\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-6, 6])\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "levels = np.logspace(0.3, 3.5, 15)\n",
    "\n",
    "CS = plt.contour(x_mesh, y_mesh, z_mesh, levels, cmap=\"viridis\")\n",
    "\n",
    "# Recast levels to new class\n",
    "\n",
    "\n",
    "ax2.clabel(CS, CS.levels)\n",
    "\n",
    "plt.xlabel(r\"$x$\", fontsize=14)\n",
    "plt.ylabel(r\"$y$\", fontsize=14)\n",
    "plt.xticks([-6, -3, 0, 3, 6])\n",
    "plt.yticks([-6, -3, 0, 3, 6])\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-6, 6])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "##  Solution\n",
    "\n",
    "</div>\n",
    "\n",
    "<details>\n",
    "  <summary>Click to expand!</summary>\n",
    "    \n",
    "If you study the contour plot on the right, you notice that the Himmelblau function has four minimas.\n",
    "\n",
    "\n",
    "The gradient of the Himmelblau function can be obtained easily as\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial x} &= 2\\times2x\\times(x^2+y-11) + 2(x+y^2-7) = \n",
    "\\boxed{4x^3+2y^2+4xy-42x-14}\\\\\n",
    "\\frac{\\partial z}{\\partial y} &= 2(x^2+y-11) + 2\\times2y\\times(x+y^2-7) = \n",
    "\\boxed{4y^3+2x^2+4xy-26y-22}\\\\\n",
    "\\end{align}\n",
    "\n",
    "    \n",
    "</details>    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below computes the gradient of the Himmelblau function.\n",
    "def Himmelblau_grad(x,y):\n",
    "    dzdx = 4*x**3 + 2*y**2 + 4*x*y -42*x -14\n",
    "    dzdy = 4*y**3 + 2*x**2 + 4*x*y -26*y -22\n",
    "    return dzdx, dzdy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the gradient at our disposal, we can minimize this function using the GD algorithm. We recall that in the GD, we update according to (custimized for our notations here)\n",
    "\\begin{align}\n",
    "x &\\gets x - \\eta\\frac{\\partial z}{\\partial x}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "y &\\gets y - \\eta\\frac{\\partial z}{\\partial y}\n",
    "\\end{align}\n",
    "\n",
    "The learning rate $\\eta$ is a hyperparameter of the algorithm. We set it to $0.01$ here. \n",
    "Below, we start from the point $\\big(x_0=0, y_0=3\\big)$ and perform 20 iterations of the GD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "lr = 0.02 # eta in formulation above\n",
    "\n",
    "x_GD, y_GD = 0,3\n",
    "x_array, y_array,  z_array = [], [], [] #to store the path taken by GD\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    tmp_z = Himmelblau_func(x_GD,y_GD)\n",
    "    \n",
    "    dzdx, dzdy = Himmelblau_grad(x_GD,y_GD)\n",
    "    x_GD = x_GD - lr*dzdx\n",
    "    y_GD = y_GD - lr*dzdy\n",
    "    \n",
    "    x_array.append(x_GD)\n",
    "    y_array.append(y_GD)\n",
    "    z_array.append(tmp_z)\n",
    "    print(f'iter:{iter:3} : z -> {z_array[-1]:.3f}, (x,y) ->({x_array[-1]:.3f},{y_array[-1]:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Seems like a very reasonable result (as the function is non-negative so its minimum cannot be less than 0!). How about we plot the steps taken by the algorithm to understand it better.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(3,figsize=(15, 6))\n",
    "contours = plt.contour(x_mesh, y_mesh, z_mesh, 6, colors='white')\n",
    "plt.clabel(contours)\n",
    "plt.imshow(z_mesh, extent=[-6, 6, -6, 6], cmap='viridis', alpha=0.90)\n",
    "plt.colorbar(ticks=np.arange(0, 2000, 400).tolist())\n",
    "for iter in range(max_iter-1):\n",
    "    xy_arrow0 = [x_array[iter], y_array[iter]]\n",
    "    xy_arrow1 = [x_array[iter+1], y_array[iter+1]]\n",
    "    plt.annotate('', xy=xy_arrow1, xytext=xy_arrow0,\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'orange', 'lw': 1},\n",
    "                   va='center', ha='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Task 2: Study the behaviour of the GD when we initialize the algorithm from the following points \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 30\n",
    "lr = 0.01 # eta in formulation above\n",
    "\n",
    "x_GD, y_GD = 0,0\n",
    "x_array, y_array,  z_array = [], [], [] #to store the path taken by GD\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    tmp_z = Himmelblau_func(x_GD,y_GD)\n",
    "    \n",
    "    dzdx, dzdy = Himmelblau_grad(x_GD,y_GD)\n",
    "    x_GD = x_GD - lr*dzdx\n",
    "    y_GD = y_GD - lr*dzdy\n",
    "    \n",
    "    x_array.append(x_GD)\n",
    "    y_array.append(y_GD)\n",
    "    z_array.append(tmp_z)\n",
    "    print(f'iter:{iter:3} : z -> {z_array[-1]:.3f}, (x,y) ->({x_array[-1]:.3f},{y_array[-1]:.3f})')\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(3,figsize=(15, 6))\n",
    "contours = plt.contour(x_mesh, y_mesh, z_mesh, 6, colors='white')\n",
    "plt.clabel(contours)\n",
    "plt.imshow(z_mesh, extent=[-6, 6, -6, 6], cmap='viridis', alpha=0.90)\n",
    "plt.colorbar(ticks=np.arange(0, 2000, 400).tolist())\n",
    "for iter in range(max_iter-1):\n",
    "    xy_arrow0 = [x_array[iter], y_array[iter]]\n",
    "    xy_arrow1 = [x_array[iter+1], y_array[iter+1]]\n",
    "    plt.annotate('', xy=xy_arrow1, xytext=xy_arrow0,\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'orange', 'lw': 1},\n",
    "                   va='center', ha='center')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 30\n",
    "lr = 0.02 # eta in formulation above\n",
    "\n",
    "x_GD, y_GD = 0,-1\n",
    "x_array, y_array,  z_array = [], [], [] #to store the path taken by GD\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    tmp_z = Himmelblau_func(x_GD,y_GD)\n",
    "    \n",
    "    dzdx, dzdy = Himmelblau_grad(x_GD,y_GD)\n",
    "    x_GD = x_GD - lr*dzdx\n",
    "    y_GD = y_GD - lr*dzdy\n",
    "    \n",
    "    x_array.append(x_GD)\n",
    "    y_array.append(y_GD)\n",
    "    z_array.append(tmp_z)\n",
    "    print(f'iter:{iter:3} : z -> {z_array[-1]:.3f}, (x,y) ->({x_array[-1]:.3f},{y_array[-1]:.3f})')\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(3,figsize=(15, 6))\n",
    "contours = plt.contour(x_mesh, y_mesh, z_mesh, 6, colors='white')\n",
    "plt.clabel(contours)\n",
    "plt.imshow(z_mesh, extent=[-6, 6, -6, 6], cmap='viridis', alpha=0.90)\n",
    "plt.colorbar(ticks=np.arange(0, 2000, 400).tolist())\n",
    "for iter in range(max_iter-1):\n",
    "    xy_arrow0 = [x_array[iter], y_array[iter]]\n",
    "    xy_arrow1 = [x_array[iter+1], y_array[iter+1]]\n",
    "    plt.annotate('', xy=xy_arrow1, xytext=xy_arrow0,\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'orange', 'lw': 1},\n",
    "                   va='center', ha='center')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD for the Logistic Regression \n",
    "\n",
    "\n",
    "\n",
    "Recall for a sample $(\\boldsymbol{x},y)$, the logistic loss is defined as\n",
    " \n",
    "\\begin{align*}\n",
    "    \\ell(y,\\hat{y}) = -y \\log(\\sigma\\big(\\boldsymbol{w}^\\top\\boldsymbol{x})\\big) -\n",
    "    (1 - y) \\log(1 - \\sigma\\big(\\boldsymbol{w}^\\top\\boldsymbol{x})\\big).\n",
    "\\end{align*}\n",
    "\n",
    "Assume $w \\in \\mathbb{R}$ and hence the model has only one parameter to be determined. THerefore, \n",
    "\\begin{align}\n",
    "\\sigma(\\boldsymbol{w}^\\top\\boldsymbol{x}) = \\frac{1}{1+\\exp(-\\boldsymbol{w}\\boldsymbol{x})}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "To update the parameters of the logistic model (i.e., $\\boldsymbol{w}$), we need to obtain\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial \\boldsymbol{w}} \\ell(y,\\hat{y}) .\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Task 3: Obtain the gradient of the logistic loss \n",
    "\n",
    "</div>\n",
    "\n",
    "The solution is provided below (not sure if this markdown feature works for VS-Code) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to expand!</summary>\n",
    "\n",
    "First note that,\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial \\boldsymbol{w}}\\ell(y,\\hat{y}) = -y \\frac{\\partial}{\\partial \\boldsymbol{w}} \\log(\\sigma\\big(\\boldsymbol{w}^\\top\\boldsymbol{x})\\big) -\n",
    "    (1 - y) \\frac{\\partial}{\\partial \\boldsymbol{w}}\\log\\big(1 - \\sigma(\\boldsymbol{w}^\\top\\boldsymbol{x})\\big)\n",
    "\\end{align*}\n",
    "    \n",
    "\n",
    "    \n",
    "First, we need to compute\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial w} \\sigma\\big(\\boldsymbol{w}\\boldsymbol{x}\\big) &= \\frac{\\partial}{\\partial w} \\frac{1}{1 + \\exp(-wx)}\\\\\n",
    "    &= \\frac{x\\exp(-wx)}{\\big(1 + \\exp(-wx)\\big)^2}\\\\\n",
    "    &= \\boxed{x\\sigma(wx)\\big(1-\\sigma(wx)\\big)}\\;.\n",
    "\\end{align*}\n",
    "\n",
    "As such,\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial \\boldsymbol{w}} \\log\\big(\\sigma(\\boldsymbol{w}\\boldsymbol{x})\\big) &= \n",
    "    \\frac{\\frac{\\partial}{\\partial w} \\sigma\\big(\\boldsymbol{w}\\boldsymbol{x}\\big)}{\\sigma\\big(\\boldsymbol{w}\\boldsymbol{x}\\big)}\\\\\n",
    "    &= \\boxed{x\\big(1-\\sigma(wx)\\big)}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial \\boldsymbol{w}} \\log\\big(1-\\sigma(\\boldsymbol{w}\\boldsymbol{x})\\big) &= \n",
    "    \\frac{\\frac{\\partial}{\\partial w} -\\sigma\\big(\\boldsymbol{w}\\boldsymbol{x}\\big)}{1 - \\sigma\\big(\\boldsymbol{w}\\boldsymbol{x}\\big)}\\\\\n",
    "    &= \\boxed{-x\\sigma(wx)}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "Putting all together,\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial \\boldsymbol{w}}\\ell(y,\\hat{y}) &= -y \\times x \\big(1-\\sigma(wx)\\big) -\n",
    "    (1 - y) \\times -x\\sigma(wx) \\\\ &=\n",
    "    -yx +yx\\sigma(wx) + x\\sigma(wx) - yx\\sigma(wx) = \\boxed{\\big(\\sigma(wx) -y\\big)x}\\;.\n",
    "\\end{align*}  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Task 4: Compare with Logistic Regression \n",
    "\n",
    "</div>\n",
    "\n",
    "Compare your result with the logistic regression algorithm, can you see any similarities? Did you just develop the logistic regression algorithm?\n",
    "\n",
    "![alt text](logistic_regression_pseudo_code.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
