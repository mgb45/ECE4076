{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Homographies for camera pose estimation \n",
    "\n",
    "This notebook shows how you can use a homography to perform camera pose estimation, in the simple case where you only undergo in plane translation and rotation. This could be useful if you have a drone taking photos from high above the ground, for image registration in remote sensing applications, or if you have a robot pointing a camera at the ceiling. This notebook refers to material covered in Week 3 and Week 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function reads an image from a URL and displays it.\n",
    "def read_image_from_url(url):\n",
    "    img = Image.open(requests.get(url, stream=True).raw)\n",
    "    return cv2.cvtColor(np.array(img),cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "im = read_image_from_url('https://www.aboriginal-art-australia.com/wp-content/uploads/2023/09/DLP148-1-294x300.jpg')\n",
    "\n",
    "# Rotate 27 degrees, tranlate -15,5 pixels\n",
    "th = 27*np.pi/180\n",
    "tx=-15\n",
    "ty =5\n",
    "M = np.array([[np.cos(th),-np.sin(th),tx],\n",
    "             [np.sin(th),np.cos(th),ty],\n",
    "              [0,0,1]])\n",
    "# Simulate the change in view that would we would undero given this transformation\n",
    "im_new = cv2.warpPerspective(im,M,im.shape,borderMode=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of our lectures speak about perspective effects, drawing on renaissance art to illustrate this. As we have learned, these create constraints that we can exploit in computer vision. I recently learned that the aerial perspective is an important stylistic and storytelling tool in [Indigenous Australian artwork](https://www.aboriginal-art-australia.com/aboriginal-art-library/aerial-perspectives-in-aboriginal-art/), with bird-eye-view imagery much more common than perspective effects. Let's look at how to exploit the constraints associated with birds-eye-view images like these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im,cmap='gray')\n",
    "plt.title('Dulcie Long Pula')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im_new,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Find the keypoints and descriptors\n",
    "kp1, des1 = orb.detectAndCompute(im,None)\n",
    "kp2, des2 = orb.detectAndCompute(im_new,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match keypoints\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1,des2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort matches\n",
    "matches = sorted(matches, key = lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw first 20 matches.\n",
    "plt.figure(figsize=(15,5))\n",
    "img3 = cv2.drawMatches(im,kp1,im_new,kp2,matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find keypoint correspondences\n",
    "X1 = np.vstack([kp1[match.queryIdx].pt for match in matches])\n",
    "X2 = np.vstack([kp2[match.trainIdx].pt for match in matches])\n",
    "\n",
    "# Estimate homograpahy using opencv - \n",
    "H, mask = cv2.findHomography(X1, X2, cv2.RANSAC, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets apply the homography to our image - in theory it should generate the second picture \n",
    "im_warp = cv2.warpPerspective(im,H,(im.shape[0],im.shape[1]))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(im)\n",
    "plt.title('Image 1')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(im_warp)\n",
    "plt.title('Image 1 warped by Homography')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(im_new)\n",
    "plt.title('Image 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing a homography\n",
    "\n",
    "A homography is a representation of a planes rotation and translation (up to scale). This can be decomposed to determine how a camera has moved from one frame to another (or how an object in a scene has moved between frames). Let's consider the case of a pure rotation in plane rotation and translation. This is the kind of motion we may see if a robot is moving and looking up at a ceiling, or in a remote sensing application where a camera is pointing down.\n",
    "\n",
    "Recall that we covered image invariances we saw that an affine map that produces rotation and translation takes the form:\n",
    "\n",
    "$\\mathbf{a} = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta & x\\\\ \\sin\\theta & \\cos\\theta & y\\\\ 0 & 0 & 1 \\end{bmatrix} $\n",
    "\n",
    "Our homography matrix should look similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('H',H)\n",
    "print('Map', M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is basically the same as the affine map we generated earlier. Lets extract the rotation angle and translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The angle of rotation is %2.2f degrees.'%(np.arctan2(H[0,1],H[0,0])*180/np.pi))\n",
    "print ('The translation is x=%2.2f, y=%2.2f pixels'%(H[0,2],H[1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. It worked, up to scale. We don't know how many pixels correspond to metres, unless we calibrate our camera."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
