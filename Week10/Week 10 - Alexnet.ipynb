{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdc181fe-769d-45fe-ab8e-45c77d647bee",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><img src=\"https://www.dropbox.com/s/roovx0dx8mpm4ep/ECE4076_banner.png?dl=1\"></div>\n",
    "\n",
    "<h1 align=\"center\"> ECE4076/5176 - Week 10 </h1>\n",
    "<h1 align=\"center\"; style=\"color: purple;\"> Here comes the AlexNet </h1>\n",
    "\n",
    "\n",
    "\n",
    "You need to have the following packages to work with this notebook\n",
    "- [pytorch](https://pytorch.org/)\n",
    "- [numpy](https://anaconda.org/anaconda/numpy)\n",
    "- [matplotlib](https://anaconda.org/conda-forge/matplotlib)\n",
    "- [pillow](https://pillow.readthedocs.io/en/stable/)\n",
    "- [requests](https://pypi.org/project/requests/)\n",
    "- [captum](https://captum.ai/)\n",
    "- [torchattacks](https://adversarial-attacks-pytorch.readthedocs.io/en/latest/index.html)\n",
    "- [scipy](https://scipy.org/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fdcd0-02d1-43d8-a545-b5ef6470642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# you may need to install teh following package\n",
    "# !pip install validators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73e23d-57da-43e2-8dee-240b052b984c",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://pytorch.org/assets/images/pytorch-logo.png\" width=\"100\" />\n",
    "\n",
    "\n",
    "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. It is nowadays the package used in academia and industry to develop deep learning models (along with TensorFlow, JAX, Keras). Below, we first check to see if we have access to an NVidia GPU or not. If you ahev a GPU, then processing required in this notebook will run much faster, if not, then no worries, you should still be able to go over all parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f6052-5357-4568-bb24-20c6cc81ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f2d48-a2c6-4d04-9238-37e456cb0b1f",
   "metadata": {},
   "source": [
    "We will use a pretrained AlexNet in this notebook. Recall that AlexNet won the imagenet challenge. \n",
    "[Imagenet](https://www.image-net.org/update-mar-11-2021.php) is a dataset with 1,000 classes, let's have a look at the object classes in imagenet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f9c4a-4822-4100-b6bf-e33981ea6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940db5a-9a9c-4bae-a8e3-f2afd7bd6bbe",
   "metadata": {},
   "source": [
    "Scroll through the classes, how many do you know?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15568e-d264-4e2f-a09c-10a68e5b97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function reads an image from a URL and displays it.\n",
    "def read_image_from_url(url):\n",
    "    img = Image.open(requests.get(url, stream=True).raw)\n",
    "    img.thumbnail((256,256), Image.LANCZOS)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7721d63f-fea0-4afb-aea9-e7274ec3fe97",
   "metadata": {},
   "source": [
    "Let's test the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0deb5d0-4ace-49bd-9706-408f7edf595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://live.staticflickr.com/7351/9416913997_df32c3b8ba_z.jpg\"\n",
    "img = read_image_from_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2dd94-a2b3-479d-8345-b75fbdd8206c",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "\n",
    "AlexNet is an 8-layer CNN, a somewhat bigger version of LeNet (see the image below) designed by [Alex Krizhevsky](https://en.wikipedia.org/wiki/Alex_Krizhevsky) in collaboration with [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever) and [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton). Like many other DL-driven models, AlexNet is just one click (well, one line of code) away from you. See the code below.  \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/1200px-Comparison_image_neural_networks.svg.png\" width=\"50%\" />\n",
    "\n",
    "Note: The number of channels may differ between the pytorch implementation and the original paper's implementation - ie. at the first convolutional layer it is 96 channels in the image, but only 64 channels in the pytorch model we are loading in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6b043-8e90-45f0-ba2b-db68d9e61800",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "alexnet.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b408ce-31af-4514-a78e-c44c583045d7",
   "metadata": {},
   "source": [
    "You should be able to recognize almost all the blocks in the network. You can use the pretrained model (which we will do) or retrain and repurpose it (which we don't do but feel free to take ECE4179/5179/6179 to learn how to do that!). To feed an image to AlexNet, we need to normalize it according to the way it was trained. For example, the image-size should be $3 \\times 224 \\times 224$. Also, RGB channels should be normalized (again, that is how the model was trained so we just need to follow accordingly). The function below takes care of that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693b47b-df04-4bbc-9ae5-a31abe316bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize_Image_AlexNet(img):\n",
    "    # First we normalize the image. This is done by scaling the image to 256x256, \n",
    "    # followed by cropping the center part (224x244), and normalizing the RGB channels \n",
    "    preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                    ])\n",
    "    # PyTorch feeds its model in a certain way (here with a batch of images). \n",
    "    # The next few lines simply take care of that\n",
    "    img_batch = preprocess(img)\n",
    "    img_batch = img_batch.unsqueeze(0).to(device)\n",
    "    return img_batch\n",
    "\n",
    "def predict_with_AlexNet(img):\n",
    "    \n",
    "    img_batch = Normalize_Image_AlexNet(img)\n",
    "    \n",
    "    # next we feed the image to AlexNet. \n",
    "    # Note that the output of AlexNet (similar to many other trained models)\n",
    "    # is the logits, showing predictions. Since we want to be able to interpret \n",
    "    # teh output as a probability vector, we need to include a softmax function at the end.\n",
    "    with torch.no_grad():\n",
    "        probabilities = F.softmax(alexnet(img_batch), dim=1)\n",
    "\n",
    "    # We are going to just focus on the top5 predictions.\n",
    "    # Again there are 1000 classes, so we really cannot bother ourselves with many classes    \n",
    "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "    top5_prob = top5_prob.to(\"cpu\").numpy().flatten()\n",
    "    top5_catid = top5_catid.to(\"cpu\").numpy().flatten()\n",
    "\n",
    "    for i, catid in enumerate(top5_catid):\n",
    "        print(categories[catid], top5_prob[i]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9900c-de73-40f1-8771-7e22f5867ce6",
   "metadata": {},
   "source": [
    "Okay. If you don't know what the previous animal was (like me), let's ask AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6b307-3247-4e87-acee-118ea8ed6507",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_with_AlexNet(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b811b-3b90-44bf-ba59-1dc49bf02d0b",
   "metadata": {},
   "source": [
    "Hmm, Marmot. Should we check? Here is the [Wiki page](https://en.wikipedia.org/wiki/Marmot). Sounds about right! \n",
    "Now, before moving to the next part, you can try AlexNet yourself. The code below will get an image from an URL and uses AlexNet to predict the class of the image. Be creative, maybe check the classes of imagenet again and try to have some fun with an image of your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4e68c-3ffd-4f31-ae14-3274a9132173",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input(\"Enter the image URL: \")\n",
    "img2 = read_image_from_url(url)\n",
    "predict_with_AlexNet(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473e77b-0fdd-4473-9f52-cf7afbe18b40",
   "metadata": {},
   "source": [
    "Okay. Now, let's try to breakdown AlexNet. Do you think AlexNet can recognize if we rotate the image? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01868a40-99c9-4cda-898a-2acea0f9ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rot = img.rotate(90)\n",
    "# plotting\n",
    "img_rot.thumbnail((256,256), Image.LANCZOS)\n",
    "plt.imshow(img_rot)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#feeding AlexNet with the rotated image\n",
    "predict_with_AlexNet(img_rot)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d11010a-d87d-4639-b14d-a5dba5259b28",
   "metadata": {},
   "source": [
    "Hold on, what happened? What is a Guenon?\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ac/Red-Eared_Guenon_at_CERCOPAN_sanctuary.JPG\" width=\"300\" />\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Red-eared_guenon), a Guenon is a type of monkey, and as we can see from the picture, they're actually quite cute. Now, you may be wondering if there's any resemblance between the rotated marmot and a Guenon monkey. I'll leave that up to you to judge.\n",
    "\n",
    "But let's move on to a more important question: if our model predicts a class, how can we identify which parts of the image are responsible for that particular prediction? This falls under the category of \"Visualization and Interpretability\" in deep learning. To answer this question, we'll use the captum package and a method called [Integrated Gradients](https://arxiv.org/abs/1703.01365). This is a popular approach for understanding CNNs. In short, the method backpropagates the gradient of a prediction to the input space, and adjusts the value of pixels to determine which ones contribute most to the prediction. This allows us to identify which parts of the image are most responsible for the prediction.\n",
    "\n",
    "Let's see how it works in practice. We need to modify our predict function to work with the requirements of the captum package. I won't go into too much detail, but feel free to study the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b90a96-a8b9-46aa-8c46-49558ab3303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "def IG_AlexNet(img):\n",
    "\n",
    "    img_batch = Normalize_Image_AlexNet(img)\n",
    "    # next we feed the image to AlexNet. \n",
    "    # Note that the output of AlexNet (similar to many other trained models)\n",
    "    # is the logits, showing predictions. Since we want to be able to interpret \n",
    "    # teh output as a probability vector, we need to include a softmax function at the end.\n",
    "    with torch.no_grad():\n",
    "        probabilities = F.softmax(alexnet(img_batch), dim=1)\n",
    "\n",
    " \n",
    "    prediction_score, pred_label_idx = torch.topk(probabilities, 1)\n",
    "\n",
    "    integrated_gradients = IntegratedGradients(alexnet)\n",
    "    ig_img_torch = integrated_gradients.attribute(img_batch, target=pred_label_idx, n_steps=200)\n",
    "    \n",
    "    \n",
    "\n",
    "    img_np = np.transpose(img_batch.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "    ig_img = np.transpose(ig_img_torch.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "    default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#000000'),\n",
    "                                                  (1, '#000000')], N=256)\n",
    "\n",
    "    _ = viz.visualize_image_attr(ig_img,img_np, method='blended_heat_map', \n",
    "                                 show_colorbar=True, sign='positive', \n",
    "                                 use_pyplot=True)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f96e56-1e92-4ec3-b993-c53dc73daff4",
   "metadata": {},
   "source": [
    "And here is the moment of truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dcc3db-c96e-4f56-b1e2-a0e4feb9a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IG_AlexNet(img_rot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e9b67d8-b9ee-4990-9b31-55fe19460580",
   "metadata": {},
   "source": [
    "Now, one possible interpretation of this, is that the examples of Guenon monkey in the dataset happen to mostly capture them standing (my poor language and engineering mind call it vertical monkey!). Both animals are furry and the form of their rotated face and nose are somehow similar (that is what I can see here at least). Anyways, this is somehow shocking. Now, do you want to know a secret? We can use the gradient to actually attack a neural network! How? Let's see. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a18c9e",
   "metadata": {},
   "source": [
    "### Activity:\n",
    "\n",
    "* Think about what you could do at training time to get around this issue? (Remember we spoke about invariances and data augmentation earlier in the unit)\n",
    "* What would you lose by doing this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395273fd-1802-4d1c-8724-79fe18305ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# being lazy, I am not going to code the attack (which is merely a for-loop) but use a package\n",
    "!pip install torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3474ba6-c373-4762-b77e-842b51523cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std) :\n",
    "        super(Normalize, self).__init__()\n",
    "        self.register_buffer('mean', torch.Tensor(mean))\n",
    "        self.register_buffer('std', torch.Tensor(std))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # Broadcasting\n",
    "        mean = self.mean.reshape(1, 3, 1, 1)\n",
    "        std = self.std.reshape(1, 3, 1, 1)\n",
    "        return (input - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "def Normalize_Image_FGSM(img):\n",
    "    # First we normalize the image. This is done by scaling the image to 256x256, \n",
    "    # followed by cropping the center part (224x244), and normalizing the RGB channels \n",
    "    preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                    ])\n",
    "    # PyTorch feeds its model in a certain way (here with a batch of images). \n",
    "    # The next few lines simply take care of that\n",
    "    img_batch = preprocess(img)\n",
    "    img_batch = img_batch.unsqueeze(0).to(device)\n",
    "    return img_batch\n",
    "\n",
    "# Adding a normalization layer for Resnet18.\n",
    "# We can't use torch.transforms because it supports only non-batch images.\n",
    "norm_layer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "AlexNet_FGSM = nn.Sequential(norm_layer, alexnet).to(device).eval()\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e8ea6-113e-4939-8d09-92a0a1e8870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchattacks\n",
    "import torchvision\n",
    "\n",
    "atk = torchattacks.FGSM(AlexNet_FGSM, eps=8/255)\n",
    "# atk = torchattacks.FFGSM(AlexNet_FGSM, eps=32/255, alpha=20/255)\n",
    "adv_images = atk(Normalize_Image_FGSM(img), torch.tensor([335])) #class Marmot\n",
    "adv_images = np.transpose(adv_images.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "adv_img = Image.fromarray(np.uint8(255*adv_images)).convert('RGB')\n",
    "plt.imshow(adv_img) #[::-1])\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "predict_with_AlexNet(adv_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf76ea9",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "That is scary! Neural networks are vulnerable to optical illusions too! We call these adversarial attacks. One way to avoid them is to incorporate these into the training process, basically as a form of data augmentation.\n",
    "\n",
    "* Edit the code above to replace with your own image (maybe think about trying something where this would really matter - eg. a face recognition system or an autonomous driving setting) and attack the classifier.\n",
    "* Try to find a more modern network online and replace Alexnet with this (eg. [ResNext](https://pytorch.org/hub/pytorch_vision_resnext/) or [MobileNet](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/)). Does this do any better on the rotated image? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
