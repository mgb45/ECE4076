{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function reads an image from a URL and displays it.\n",
    "def read_image_from_url(url):\n",
    "    img = Image.open(requests.get(url, stream=True).raw)\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, Let's use a pre-trained network as a feature extractor. We'll do this by slicing the last layers off Alexnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the final classifier projection with nothing\n",
    "alexnet.classifier[-1] = torch.nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's gone now\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get some images\n",
    "url = \"https://static.wikia.nocookie.net/waldo/images/9/9d/Character.Waldo.jpg/revision/latest/top-crop/width/200/height/150?cb=20071001045624\"\n",
    "wally = read_image_from_url(url)\n",
    "\n",
    "url = \"https://static.wikia.nocookie.net/waldo/images/4/45/Character.Odlaw.jpg/revision/latest/top-crop/width/200/height/150?cb=20071001053010\"\n",
    "odlaw = read_image_from_url(url)\n",
    "\n",
    "url = \"https://static.wikia.nocookie.net/waldo/images/3/3e/Character.Wenda.jpg/revision/latest/top-crop/width/200/height/150?cb=20071001044014\"\n",
    "wenda = read_image_from_url(url)\n",
    "\n",
    "url = \"https://static.wikia.nocookie.net/waldo/images/9/9d/Character.Waldo.jpg/revision/latest/scale-to-width-down/150?cb=20071001045624\"\n",
    "test_image = read_image_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing needed for alexnet\n",
    "preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(wally)\n",
    "plt.title('Wally')\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(odlaw)\n",
    "plt.title('Odlaw')\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(wenda)\n",
    "plt.title('Wenda')\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(test_image)\n",
    "plt.title('Test image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract features using alexnet\n",
    "img_batch = preprocess(wally).unsqueeze(0)\n",
    "feature_wally = alexnet(img_batch)\n",
    "img_batch = preprocess(odlaw).unsqueeze(0)\n",
    "feature_odlaw = alexnet(img_batch)\n",
    "img_batch = preprocess(wenda).unsqueeze(0)\n",
    "feature_wenda = alexnet(img_batch)\n",
    "img_batch = preprocess(test_image).unsqueeze(0)\n",
    "feature_test = alexnet(img_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What dimension are the features? Look above to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_wally.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's use these features as descriptors and see which one our test image is most similar to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Wally distance: ',torch.sum((feature_wally-feature_test)**2).item())\n",
    "print('Wenda distance: ',torch.sum((feature_wenda-feature_test)**2).item())\n",
    "print('Odlaw distance: ',torch.sum((feature_odlaw-feature_test)**2).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean distances are weird in high dims, so more common to use cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Wally similarity: ',torch.nn.CosineSimilarity()(feature_wally,feature_test).item())\n",
    "print('Wenda similarity: ',torch.nn.CosineSimilarity()(feature_wenda,feature_test).item())\n",
    "print('Odlaw similarity: ',torch.nn.CosineSimilarity()(feature_odlaw,feature_test).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's cool. Our test picture of Wally is closest to the image of his face! Let's make it harder and see if we can find wally in a more challenging image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://static.boredpanda.com/blog/wp-content/uploads/2020/03/where-is-waldo-coronavirus-edition-book-5-5e73257690fa2__700.jpg'\n",
    "base_image = read_image_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.imshow(base_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to divide this image into pn x pn patches, extract features in each, and compute the similarity between this and Wally's face feature above. I'll store this in a heatmap to overlay on the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = 8\n",
    "dists = []\n",
    "for i in np.linspace(0,base_image.shape[0],pn+1,dtype=int)[:-1]:\n",
    "    for j in np.linspace(0,base_image.shape[1],pn+1,dtype=int)[:-1]:\n",
    "        w = int(base_image.shape[0]/pn)\n",
    "        h = int(base_image.shape[1]/pn)\n",
    "        \n",
    "        patch = base_image[i:i+w,j:j+h,:]\n",
    "        \n",
    "        img_batch = preprocess(patch).unsqueeze(0)\n",
    "        patch_feature = alexnet(img_batch)\n",
    "\n",
    "        distance = torch.nn.CosineSimilarity()(patch_feature,feature_wally).item()\n",
    "        \n",
    "        dists.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.imshow(base_image,extent=[0,pn,pn,0])\n",
    "plt.imshow(np.array(dists).reshape(pn,pn),alpha=0.5,extent=[0,pn,pn,0])\n",
    "idx = np.argmax(np.array(dists).reshape(pn,pn))\n",
    "plt.plot(idx%pn+0.5,idx//pn+0.5,'rx',markersize=25)\n",
    "plt.colorbar(label='Similarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we found Wally! This quick similarity measure between features shows us a couple things. \n",
    "\n",
    "1. The network has learned to encode our image into a feature vector that retains useful information about images. \n",
    "2. This feature vector appears to have some invariances built in.\n",
    "3. Even though the network wasn't trained for this (Wally isn't in imagenet classes), the representation is useful. \n",
    "\n",
    "We could re-use this, eg. by training a logistic regression classifier like we did in week 8, to get a powerfull recognition model. Or we could fine-tune the existing model on a new dataset, to improve the entire model. This reduces some of the data requirements we may have if training from scratch.\n",
    "\n",
    "### Activity:\n",
    "\n",
    "* Try change the patch number/size and see how this affects the distance measure.\n",
    "* Try find a harder image to test with online - it probably won't work. \n",
    "\n",
    "Next week we will look at models that do detection by region proposal, a much faster approach than the search we just did. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece4076_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
