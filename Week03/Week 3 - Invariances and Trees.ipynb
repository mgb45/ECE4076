{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Invariances and Asymmetric feature matching\n",
    "\n",
    "\n",
    "In our previous notebook, we saw that patch matching was complicated by self-similar appearances, viewpoint and lighting variation. Models that can work across all these variations are often called invariant. In earlier work on computer vision, a lot of work was conducted to build image patch descriptors that are invariant to these variations, for example the [Scale Invariant Feature Transform (SIFT)](https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html). More recently, it is common to use image descriptors learned by deep learning approaches for matching. \n",
    "\n",
    "Today though, we will explore a fast tree-based method for patch matching, that is particularly popular in visual odometry, where we use keypoints detected in multiple frames to infer how a camera has moved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from IPython import display\n",
    "\n",
    "from jupyterquiz import display_quiz\n",
    "import json\n",
    "with open(\"./Questions_week_3.json\", \"r\") as file:\n",
    "    questions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in our two drone images again\n",
    "im_1 = cv2.cvtColor(cv2.resize(cv2.imread('../test_images/Drone_1.jpg'),(400,400)), cv2.COLOR_BGR2GRAY)\n",
    "im_2 = cv2.cvtColor(cv2.resize(cv2.imread('../test_images/Drone_2.jpg'),(400,400)), cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll grab some Harris corners\n",
    "harris_1 = cv2.cornerHarris(im_1,blockSize=2,ksize=3,k=0.04)\n",
    "kp_1 = harris_1 > 0.05*harris_1.max() \n",
    "kpx_1, kpy_1 = np.where(kp_1)\n",
    "\n",
    "harris_2 = cv2.cornerHarris(im_2,blockSize=2,ksize=3,k=0.04)\n",
    "kp_2 = harris_2 > 0.05*harris_2.max() \n",
    "kpx_2, kpy_2 = np.where(kp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(im_1,cmap='gray')\n",
    "plt.title('Image 1')\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(kp_1,cmap='gray')\n",
    "plt.title('Harris corners 1')\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(im_2,cmap='gray')\n",
    "plt.title('Image 2')\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(kp_2,cmap='gray')\n",
    "plt.title('Harris corners 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(im,x,y,w=10):\n",
    "    return im[x-w:x+w,y-w:y+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of invariances\n",
    "\n",
    "First though, let's look at a few different invariances we might need to deal with. We will apply a few affine transformations to the image patch to illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_x = kpx_1[20]\n",
    "pos_y = kpy_1[20]\n",
    "test_patch = get_patch(im_1,pos_x,pos_y,40)\n",
    "plt.imshow(test_patch,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate 5 pixels\n",
    "M = np.array([[1,0,5.0],[0,1,5.0]])\n",
    "translated_patch = cv2.warpAffine(test_patch,M,test_patch.shape,borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "# Rotate 45 degrees\n",
    "M = cv2.getRotationMatrix2D((test_patch.shape[0]/2,test_patch.shape[1]/2),45,1)\n",
    "rotated_patch = cv2.warpAffine(test_patch,M,test_patch.shape,borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "# Scaling\n",
    "M = np.array([[1.5, 0  , 0],[0,   1.8, 0],[0,   0,   1]])\n",
    "scaled_patch = cv2.warpPerspective(test_patch,M,(test_patch.shape[0],test_patch.shape[0]),borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "# Shearing\n",
    "M = np.array([[1, 0.5, 0],[0, 1  , 0],[0, 0  , 1]])\n",
    "sheared_patch = cv2.warpPerspective(test_patch,M,test_patch.shape,borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,5,1)\n",
    "plt.imshow(test_patch,cmap='gray')\n",
    "plt.title('Original')\n",
    "plt.subplot(1,5,2)\n",
    "plt.imshow(translated_patch,cmap='gray')\n",
    "plt.title('Translated')\n",
    "plt.subplot(1,5,3)\n",
    "plt.imshow(rotated_patch,cmap='gray')\n",
    "plt.title('Rotated')\n",
    "plt.subplot(1,5,4)\n",
    "plt.imshow(scaled_patch,cmap='gray')\n",
    "plt.title('Scaling')\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(sheared_patch,cmap='gray')\n",
    "plt.title('Shearing')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations illustrate a few of the different appearances we may encounter when trying to match patches across viewpoints. No wonder the SSD error wasn't very good at matching. We need matching algorithms that can cope with these appearance changes.\n",
    "\n",
    "### Learning from synthetic data\n",
    "\n",
    "These warping operations - which we will cover in more detail next week when we tackle Homographies, are really useful to generate example images for methods that try to learn from data. From a single image, we can generate many different variations to learn from. This is a common strategy in deep learning approaches, and called data augmentation. As an example, if we were to teach a neural network that this image is a picture of a propeller, we could generate 100s of examples of this image with labels, and have our network learn to be invariant to these appearance variations.\n",
    "\n",
    "To get a clearer idea of how this might work, let's implement an assymetric feature matching search. We'll build on the patch matching code used last week, and only consider rotation and translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to randomly shift and rotate an image patch\n",
    "def random_warp(patch):\n",
    "    \n",
    "    # Translate randomly\n",
    "    d = 5*np.random.randn()\n",
    "    M = np.array([[1,0,d],[0,1,d]])\n",
    "    patch = cv2.warpAffine(patch,M,patch.shape,borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Rotate randomly\n",
    "    theta = 30*np.random.randn()\n",
    "    M = cv2.getRotationMatrix2D((patch.shape[0]/2,patch.shape[1]/2),theta,1)\n",
    "    patch = cv2.warpAffine(patch,M,patch.shape,borderMode=cv2.BORDER_REPLICATE)\n",
    "    \n",
    "    return patch\n",
    "    \n",
    "# A function to compute sum of squared distances between patches\n",
    "def ssd(patch_1,patch_2):\n",
    "    \n",
    "    if (patch_1.shape != patch_2.shape): #Ignore patches around the edges \n",
    "        return 1e15\n",
    "    else:\n",
    "        return np.sum((patch_1-patch_2)**2)\n",
    "    \n",
    "# Assymetric feature matching - returns the minimum distance across view variations\n",
    "def find_minimum_distance_across_viewpoints(patch_1,patch_2,N=100):\n",
    "    \n",
    "    min_dist = 1e15\n",
    "    if (patch_1.shape != patch_2.shape): #Ignore patches around the edges \n",
    "        return min_dist\n",
    "    else:\n",
    "        for j in range(N):\n",
    "            random_patch_2 = random_warp(patch_2)\n",
    "            dist = ssd(patch_1,random_patch_2)\n",
    "\n",
    "            if (dist < min_dist):\n",
    "\n",
    "                min_dist = dist\n",
    "    return min_dist\n",
    "\n",
    "# Finds the closest patch to a list of patches, with assymetric feature matching\n",
    "def find_closest_patch(im_to_search, kp_x_to_search, kp_y_to_search, query_patch):\n",
    "    \n",
    "    min_dist = 1e15\n",
    "    min_dist_idx = 0\n",
    "    # Loop over all corners\n",
    "    for i in range(len(kp_x_to_search)):\n",
    "        \n",
    "        pos_x = kp_x_to_search[i]\n",
    "        pos_y = kp_y_to_search[i]\n",
    "        test_patch = get_patch(im_to_search,pos_x,pos_y,w=int(query_patch.shape[0]/2))\n",
    "        \n",
    "        # Compare 100 different patch views and return the minimum distance\n",
    "        dist = find_minimum_distance_across_viewpoints(test_patch,query_patch,N=10)\n",
    "        \n",
    "        if dist <= min_dist:\n",
    "            min_dist = dist\n",
    "            min_dist_idx = i\n",
    "            \n",
    "    pos_x = kp_x_to_search[min_dist_idx]\n",
    "    pos_y = kp_y_to_search[min_dist_idx]\n",
    "        \n",
    "    return get_patch(im_to_search,pos_x,pos_y), min_dist_idx, min_dist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all keypoints in image 2, and find closest match in image 1\n",
    "match_idxs = []\n",
    "match_dists = []\n",
    "for j in range(len(kpx_2)):\n",
    "    pos_x = kpx_2[j]\n",
    "    pos_y = kpy_2[j]\n",
    "    test_patch = get_patch(im_2,pos_x,pos_y)\n",
    "    best_patch, patch_idx, patch_dist = find_closest_patch(im_1,kpx_1,kpy_1,test_patch)\n",
    "    match_idxs.append(patch_idx)\n",
    "    match_dists.append(patch_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches(kpx_1,kpy_1,kpx_2,kpy_2,jump=100):\n",
    "    plt.plot([kpy_1,kpy_2+jump],[kpx_1,kpx_2],'b',alpha=0.2)\n",
    "\n",
    "im_both = np.hstack((im_2,im_1)) # Concatenate images\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(im_both,cmap='gray')\n",
    "for j in range(len(match_idxs)):\n",
    "    x_2 = kpx_2[j]\n",
    "    y_2 = kpy_2[j]\n",
    "    \n",
    "    x_1 = kpx_1[match_idxs[j]]\n",
    "    y_1 = kpy_1[match_idxs[j]]\n",
    "    if match_dists[j] < 0.8*max(match_dists): # Only show matches below some distance threshold\n",
    "        plot_matches(x_2,y_2,x_1,y_1,jump=im_1.shape[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we definitely got more matches than we had last week (quite a few incorrect ones because this drone has a lot of self-similar patches). These additional matches occur because we have added rotational invariance to our feature descriptor.\n",
    "\n",
    "### Activity 1\n",
    "\n",
    "- Count the number of loops we used for asymmetric feature matching.\n",
    "- If there are K1 keypoints in the first image, K2 keypoints in the second image, and N different viewpoint variations, how many distances does the nearest neighbour search need to compute?\n",
    "- Increase the value of N, what does this do to the speed of the search?\n",
    "- How could you speed up this search, try to list three strategies from the lecture videos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(questions[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRIEF - Binary Robust Independent Elementary Features \n",
    "\n",
    "The approach above is very slow, and we discussed a number of methods in the lectures to speed this up. Most of these relied on incurring a larger initial or upfront cost to reorganise our data for fast searching, or to train a model (eg. a decision tree) that makes it easier to rapidly search by asking targeted questions about our image patches.\n",
    "\n",
    "This approach can be problematic for for real-time systems, so let's look at a quick and dirty approach. Let's write a class to implement the basic idea behind BRIEF, which is very very fast. We won't consider assymetric feature matching here, but it's easy to expand to this too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BriefDescriptor:\n",
    "    \n",
    "    def __init__(self,width=10,num_tests=64):\n",
    "        \n",
    "        # First, generate a random set of coordinate pairs to compare image intensity\n",
    "        self.coords = np.random.randint(0,width,(num_tests,4))\n",
    "        \n",
    "    # We'll return the comparison result, and add some dodgy error handling \n",
    "    def binary_comparison(self,patch,coord):\n",
    "        try:\n",
    "            return (patch[coord[0],coord[1]] > patch[coord[2],coord[3]])\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    # Brief builds a code or descriptor for an image patch using a number of test outcomes\n",
    "    def get_binary_code(self,patch):\n",
    "        \n",
    "        code = [self.binary_comparison(patch,self.coords[i,:]) for i in range(self.coords.shape[0])]\n",
    "        \n",
    "        return np.array(code)\n",
    "\n",
    "# We can match these code vectors very quickly, using some neat numpy tricks and a hamming distance\n",
    "def FastMatching(codes_1, codes_2):\n",
    "    \n",
    "        c1 = np.array(codes_1)[np.newaxis,:,:] # reshape to 1 x Nkeypoints X descriptor dimension\n",
    "        c2 = np.array(codes_2)[:,np.newaxis,:] # reshape to Nkeypoints x 1 x descriptor dimension\n",
    "        \n",
    "        pairwise_hamming_distance = np.sum(c1!=c2,axis=-1) # take advantage of numpy broadcasting\n",
    "        \n",
    "        return np.argmin(pairwise_hamming_distance,axis=-1), np.min(pairwise_hamming_distance,axis=-1) #return the location of the minimum distance\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = 10\n",
    "detector = BriefDescriptor(width=pw,num_tests=64)\n",
    "\n",
    "# extract binary descriptors for each patch\n",
    "codes_im_1 = [detector.get_binary_code(get_patch(im_1,kpx_1[j],kpy_1[j],pw)) for j in range(len(kpx_1))] \n",
    "codes_im_2 = [detector.get_binary_code(get_patch(im_2,kpx_2[j],kpy_2[j],pw)) for j in range(len(kpx_2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_idxs, min_distances = FastMatching(codes_im_1,codes_im_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches(kpx_1,kpy_1,kpx_2,kpy_2,jump=100):\n",
    "    plt.plot([kpy_1,kpy_2+jump],[kpx_1,kpx_2],'b',alpha=0.2)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "im_both = np.hstack((im_2,im_1)) # Concatenate images\n",
    "plt.imshow(im_both,cmap='gray')\n",
    "for j in np.argsort(min_distances)[:50]: # Show only the top 50 matches\n",
    "    x_2 = kpx_2[j]\n",
    "    y_2 = kpy_2[j]\n",
    "    \n",
    "    x_1 = kpx_1[match_idxs[j]]\n",
    "    y_1 = kpy_1[match_idxs[j]]\n",
    "    \n",
    "    plot_matches(x_2,y_2,x_1,y_1,jump=im_1.shape[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets study how the brief detector distance changes as a function of viewpoint effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = 10\n",
    "detector = BriefDescriptor(width=pw,num_tests=64)\n",
    "\n",
    "pos_x = kpx_1[20]\n",
    "pos_y = kpy_1[20]\n",
    "test_patch = get_patch(im_1,pos_x,pos_y,pw)\n",
    "plt.imshow(test_patch,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_patch(patch,theta):\n",
    "\n",
    "    M = cv2.getRotationMatrix2D((patch.shape[0]/2,patch.shape[1]/2),theta,1)\n",
    "    return cv2.warpAffine(patch,M,patch.shape,borderMode=cv2.BORDER_REPLICATE)\n",
    "    \n",
    "rotated_patches = [rotate_patch(test_patch,theta) for theta in np.linspace(-180,180,10)]\n",
    "initial_code = detector.get_binary_code(test_patch)\n",
    "rotated_codes = [detector.get_binary_code(rotated_patch) for rotated_patch in rotated_patches]\n",
    "\n",
    "c1 = np.array(initial_code)[np.newaxis,:] # reshape to 1 x Nkeypoints X descriptor dimension\n",
    "c2 = np.array(rotated_codes)[:,np.newaxis] # reshape to Nkeypoints x 1 x descriptor dimension\n",
    "        \n",
    "hamming_distance = np.sum(c1!=c2,axis=-1) # take advantage of numpy broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "for j in range(len(rotated_patches)):\n",
    "    plt.subplot(2,len(rotated_patches),j+1)\n",
    "    plt.imshow(rotated_patches[j],cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.linspace(-180,180,10),hamming_distance)\n",
    "plt.xlabel('Degrees')\n",
    "plt.ylabel('Hamming distance')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2\n",
    "\n",
    "Modify the code above to change the number of tests used by the BRIEF Detector (the descriptor length) and the patch width. \n",
    "\n",
    "* How does this change the error as a function of rotation angle?\n",
    "* Why is this happening?\n",
    "\n",
    "\n",
    "\n",
    "### Ok Lets fix this issue by correcting for rotation\n",
    "\n",
    "We are going to use the image gradient to find the dominant orientation and reorient our patches to correct for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz([questions[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This \n",
    "def get_dominant_angle(patch):\n",
    "    \n",
    "    patch = patch.astype(np.uint8)\n",
    "    gx = cv2.Sobel(patch, cv2.CV_32F, 1, 0, ksize=1)\n",
    "    gy = cv2.Sobel(patch, cv2.CV_32F, 0, 1, ksize=1)\n",
    "    angles = cv2.phase(gx,gy)\n",
    "    hist,bins = np.histogram(angles,np.linspace(0,2*np.pi,12))\n",
    "    \n",
    "    return bins[np.argmax(hist)]*180/np.pi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_code = detector.get_binary_code(rotate_patch(test_patch,get_dominant_angle(test_patch)))\n",
    "reoriented_patches = [rotate_patch(rotated_patch,get_dominant_angle(rotated_patch)) for rotated_patch in rotated_patches]\n",
    "reoriented_codes = [detector.get_binary_code(patch) for patch in reoriented_patches]\n",
    "\n",
    "c1 = np.array(initial_code)[np.newaxis,:] # reshape to 1 x Nkeypoints X descriptor dimension\n",
    "c2 = np.array(reoriented_codes)[:,np.newaxis] # reshape to Nkeypoints x 1 x descriptor dimension\n",
    "        \n",
    "new_hamming_distance = np.sum(c1!=c2,axis=-1) # take advantage of numpy broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "for j in range(len(reoriented_patches)):\n",
    "    plt.subplot(2,len(reoriented_patches),j+1)\n",
    "    plt.imshow(reoriented_patches[j],cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.linspace(-180,180,10),hamming_distance,label='No Reorientation (Brief)')\n",
    "plt.plot(np.linspace(-180,180,10),new_hamming_distance,label='Reorientation (~Oriented-Brief)')\n",
    "plt.xlabel('Degrees')\n",
    "plt.ylabel('Hamming distance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, the reorientation makes our matching error much more robust to rotations. The images above show the orientation detection generally works as expected too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up\n",
    "\n",
    "Ok, so we now have a very rough and noisy technique that let's us locate a patch in an image. We've used a lot of tools to get this far:\n",
    "\n",
    "- We've looked at convolutional filters to enhance or suppress unecessary information in an image\n",
    "- We've covered keypoint and patch representations in images\n",
    "- We've covered data augmentation and image invariances\n",
    "- We've encountered some of the computational complexities of search or matching\n",
    "- We've applied the idea of compressing an image or patch to a vector or descriptor to compactly capture useful information for a downstream task (eg. patch matching)\n",
    "\n",
    "Next week, we will start to look at some geometric properties of cameras, and how we can use keypoint matches to figure out the camera properties. We also encounter a robust model fitting approach that can handle outliers, like the eroneous matches we see above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece4076_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
